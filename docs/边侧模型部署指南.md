# 边侧模型部署指南

## 总则

### 目的

* 本文档的目的是指导在BM1684X芯片上部署bmodel模型，帮助使用者可以快速完成模型推理。本文档适用于BM1684X模型部署以及常见问题的排查。

### 相关资料

* 模型移植：详见[开源算法生态构建与模型移植](https://wiki.sophgo.com/pages/viewpage.action?pageId=137013780)COP。
* bmodel结构：bmodel是tpu-mlir编译的二进制模型二进制文件，可包含多net多stage多个subnet，以及支持动态网络。结构图见：[bmodel结构](https://wiki.sophgo.com/pages/viewpage.action?pageId=173660847)。

## 基于bmruntime部署

`bmruntime`包含在`libsophon`项目中。soc设备上，在`/opt/sophon/libsophon-current/`目录下已经预装，可以直接使用。

[LLM-TPU项目](https://github.com/sophgo/LLM-TPU)是使用`bmruntime`部署bmodel的一个代表，该项目通过`pybind11`编译出可被python调用的`.so`文件。使用时需要现编译，编译出的`.so`绑定python版本。

## 基于[sail](https://github.com/sophgo/sophon-sail)部署

是通过`pybind11`对`bmruntime`等进行封装而来的python接口，打通了视频图像的解码，图像的预处理，模型的推理，且都使用了硬件加速，对于cv类模型的部署有益。也可用于部署LLM，参考COP[基于sophon-sail的LLM优化指南](https://wiki.sophgo.com/pages/viewpage.action?pageId=156455842)。

绑定python版本，预编译的版本有python3.8和python3.10。SAIL依赖libsophon、sophon-ffmpeg、sophon-opencv，如果其中任意一个模块进行的版本的更新,SAIL都需要重新进行编译。用户自行编译时，有一定的门槛。

快速推理代码：

```python
import sophon.sail as sail

# 封装成类似于onnxruntime的使用方式
class EngineOV:
    def __init__(self, model_path, device_id=0) :r
        self.model_path = model_path
        self.device_id = device_id
        self.model = sail.Engine(model_path, device_id, sail.IOMode.SYSIO)
        sail.set_print_flag(False)
        self.graph_name = self.model.get_graph_names()
        self.input_name = []
        self.output_name = []
        for name in self.graph_name:
            self.input_name.append(self.model.get_input_names(name))
            self.output_name.append(self.model.get_output_names(name))

    def __str__(self):
        return "EngineOV: model_path={}, device_id={}".format(self.model_path,self.device_id)
    
    # 适用于combine多net的bmodel
    def __call__(self, args, net_name=None, net_num=0):
        if isinstance(args, list):
            values = args
        elif isinstance(args, dict):
            values = list(args.values())
        else:
            raise TypeError("args is not list or dict")
        args = {}
        if net_name is not None:
            graph_name = net_name
            input_name = self.model.get_input_names(net_name)
            output_name = self.model.get_output_names(net_name)
        elif net_num is not None:
            graph_name = self.graph_name[net_num]
            input_name = self.input_name[net_num]
            output_name = self.output_name[net_num]
        else:
            input_name = self.input_name[0]
            output_name = self.output_name[0]

        for i in range(len(values)):
            args[input_name[i]] = values[i]

        output = self.model.process(graph_name, args)
        res = []
        for name in output_name:
            res.append(output[name])
        return res

# 推理使用
model = EngineOV("path/to/model.bmodel")
result = model([input_np_array])[0]
```

## 基于[tpu_perf](http://172.28.142.50:8090/nntoolchain/tpu-perf)部署

tpu_perf不依赖python版本，可直接安装wheel包使用。只支持单个net单个stage的bmodel。使用指南COP: [tpu-perf 使用指南-v1.1.2](https://wiki.sophgo.com/pages/viewpage.action?pageId=151382331)

快速推理代码：

```python
from tpu_perf.infer import SGInfer

class EngineOV:
    def __init__(self, model_path="", batch=1, device_id=0):
        self.model = SGInfer(model_path, batch=batch, devices=[device_id])

    def __str__(self):
        return "EngineOV: model_path={}, device_id={}".format(self.model_path, self.device_id)

    def __call__(self, args):
        if isinstance(args, list):
            values = args
        elif isinstance(args, dict):
            values = list(args.values())
        else:
            raise TypeError("args is not list or dict")
        task_id = self.model.put(*values)
        task_id, results, valid = self.model.get()
        return results

model = EngineOV("path/to/model.bmodel")
result = model([input_np_array])[0]
```

## 使用[untool](https://www.modelscope.cn/models/wlc952/UnTool)部署

UnTool 是一个用于 BM1684X 芯片的模型推理工具包，提供高效的模型部署和执行功能。该框架支持通用模型推理以及大语言模型(LLM)的推理，具有以下特点：

- 最小化依赖，重写bmruntime;
- 优化内存管理，统一将host的mem和device的mem管理为tensor，快速实现两端的数据同步；支持numpy，共享numpy数据的内存地址，零copy；
- 提供C++和Python双语言接口；使用ctypes链接c++和python，脱离版本依赖；
- 支持多nets多stages以及dynamic subnets；支持通用模型和LLM模型
- 融合tpukernel，将算子和tensor绑定，更方便debug和调用算子。（todo）

该项目发布了wheel预编译包，不受python版本限制，适用于aarch64/x86平台，支持soc/pcie模式。推荐直接安装预编译包来使用：`pip install untool`。

### 使用示例

**参数传递**：

```python
import argparse
parser = argparse.ArgumentParser()
parser.add_argument('-m', '--model_path', type=str, required=True, help='path to the bmodel file')
parser.add_argument('-t', '--tokenizer_path', type=str, required=True, help='path to the tokenizer file')
parser.add_argument('-d', '--devid', type=int, default=0, help='device ID to use')
parser.add_argument('--generation_mode', type=str, choices=["greedy", "penalty_sample"], default="greedy", help='mode for generating next token')
parser.add_argument('--enable_history', action='store_true', help="if set, enables storing of history memory")
args = parser.parse_args()
```

**接口使用**：

* 接口1：适用于LLM或多模态LLM，`untool.LLMBaseModel`封装，便于修改。

```python
from untool import LLMBasePipeline, MiniCPMVPipeline

# pipline = LLMBasePipeline(args) # 常规LLM, 如Qwen2.5
pipline = MiniCPMVPipeline(args) # 多模态LLM, 如MiniCPMV2.6、MiniCPM3o
pipline.chat()
```

* 接口2：适用于LLM，cpp底层封装，推理速度更快，不便修改。
  
```python
from untool import EngineLLM

engine = EngineLLM(args)
engine.chat()
```

* 接口3：适用于cv模型等

```python
from untool import EngineOV

model = EngineOV("path/to/model.bmodel", device_id=0)
result = model([input_np_array])[0]
```

## 基于fastapi创建llm api服务

在实际生产环境中，推荐通过API方式对外提供LLM服务。下面以FastAPI为例，分别介绍常规LLM和多模态LLM的API服务搭建流程。

### 常规LLM API服务

#### 常规LLM API服务的请求格式

请求体为JSON，主要字段如下：

```json
{
  "messages": [
    {"role": "user", "content": "你好，介绍一下你自己"}
  ],
  "stream": false
}
```

* `messages`：对话历史，列表形式，每条包含`role`（如"user"、"assistant"）和`content`（消息内容）。
* `stream`：是否流式返回，布尔值，默认为`false`。

#### 常规LLM API服务的响应格式

响应体为JSON，主要字段如下：

```json
{
  "object": "chat.completion",
  "created": 1714450000,
  "model": "qwen2.5-3b_int4_seq4096_1dev",
  "choices": [
    {
      "delta": {
        "role": "assistant",
        "content": "你好，我是一个AI助手，很高兴为你服务。"
      }
    }
  ]
}
```

* `object`：对象类型，固定为`chat.completion`。
* `created`：时间戳。
* `model`：模型名称。
* `choices`：回复内容，列表形式，`delta`字段包含角色和回复文本。

流式返回时，每次推送的数据格式与上述`choices`结构一致，采用Server-Sent Events（SSE）协议。

#### 搭建常规LLM API服务（基于fastapi和untool）

##### 路由与模型初始化

```python
from fastapi import FastAPI
from fastapi.responses import JSONResponse, StreamingResponse
import argparse
import time
from pydantic import BaseModel, Field
import json
from untool import LLMBasePipeline

app = FastAPI()

# 直接定义模型路径
sdk_abs_path = "/your/abs/path"  # 请替换为实际路径
model_path = f'{sdk_abs_path}/qwen2.5-3b_int4_seq4096_1dev.bmodel'
confg_path = f'{sdk_abs_path}/token_config'

# 启动时加载模型
class LLMService:
    def __init__(self):
        args = argparse.Namespace(
            model_path     = model_path,
            tokenizer_path = confg_path,
            devid          = 0,
            generation_mode= "greedy",
            enable_history = False,
        )
        self.llm_model = LLMBasePipeline(args)

llm_service = LLMService()
```

##### 聊天接口实现

```python
class ChatRequest(BaseModel):
    messages: list = Field([{"role":"user","content":"hello"}], description="Chat history")
    stream: bool = Field(False, description="Stream response")

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    slm = router.llm_model

    history = [{"role": "system", "content": "You are a helpful assistant."}]
    history += request.messages

    input_ids = slm.tokenizer.apply_chat_template(history, tokenize=True, add_generation_prompt=True)

    if request.stream:
        def generate_responses():
            token = slm.model.forward_first(input_ids)
            output_tokens = []
            while token != slm.EOS and slm.token_length < slm.model.SEQLEN:
                output_tokens.append(token)
                word = slm.tokenizer.decode(output_tokens, skip_special_tokens=True)
                if "�" not in word:
                    if len(output_tokens) == 1:
                        pre_word = word
                        word = slm.tokenizer.decode([token, token], skip_special_tokens=True)[len(pre_word):]
                    data = {"object":"chat.completion",
                            "created":int(time.time()),
                            "model":request.model,
                            "choices": [{"delta": {"role": "assistant", "content": word}}]}
                    yield f"data:{json.dumps(data)}\n\n"
                    output_tokens = []
                token = slm.model.forward_next()
        return StreamingResponse(generate_responses(), media_type="text/event-stream")
    else:
        token = slm.model.forward_first(input_ids)
        output_tokens = []
        while token != slm.EOS and slm.model.token_length < slm.model.SEQLEN:
            output_tokens.append(token)
            token = slm.model.forward_next()
        answer = slm.tokenizer.decode(output_tokens)
        data = {"object":"chat.completion",
                "created":int(time.time()),
                "model":request.model,
                "choices": [{"delta": {"role": "assistant", "content": answer}}]}
        return JSONResponse(data)
```

### 多模态LLM API服务搭建

#### 多模态LLM API服务的请求格式

请求体为JSON，主要字段如下：

```json
{
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "请描述这张图片"},
        {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD..."}}
      ]
    }
  ],
  "stream": false
}
```

* `messages`：对话历史，`content`字段可为字符串或列表，列表中可包含文本（`type: text`）和图片（`type: image_url`，支持base64或http(s)图片链接）。
* `stream`：是否流式返回，布尔值。

#### 多模态LLM API服务的响应格式

响应体为JSON，主要字段如下：

```json
{
  "object": "chat.completion",
  "created": 1714450000,
  "model": "MiniCPM-V2.6",
  "choices": [
    {
      "delta": {
        "role": "assistant",
        "content": "这是一张风景图片，画面中有蓝天和白云。"
      }
    }
  ]
}
```

- 字段含义与常规LLM一致，`model`字段为多模态模型名称。

流式返回时，每次推送的数据格式与上述`choices`结构一致，采用SSE协议。

#### 搭建多模态LLM API服务（基于fastapi和untool）

##### 聊天接口实现（支持图片）

```python
import base64
from io import BytesIO
from PIL import Image
import uuid
import os
from untool import MiniCPMVPipeline

app_name = "minicpmv"
model_path = f"{sdk_abs_path}/minicpm3o_bm1684x_int4_seq1024.bmodel"
confg_path = f"{sdk_abs_path}/processor_config"

class VLMService:
    def __init__(self):
        args = argparse.Namespace(
            model_path     = model_path,
            tokenizer_path = confg_path,
            devid          = 0,
            generation_mode= "greedy",
            enable_history = False,
        )
        self.vlm_model = MiniCPMVPipeline(args)

vlm_service = VLMService()

class ChatRequest(BaseModel):
    messages: list = Field([{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"hello"}], description="Chat history")
    stream: bool = Field(False, description="Stream response")

@router.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    slm = vlm_service.vlm_model
    slm.input_str = ''
    slm.image_str = []
    tmp_dir = "./tmpdir"
    os.makedirs(tmp_dir, exist_ok=True)
    tmp_dir = os.path.abspath(tmp_dir)

    # 解析消息内容，支持文本和图片
    if isinstance(request.messages[-1]['content'], list):
        content = request.messages[-1]['content']
        for x in content:
            if x['type'] == 'text':
                slm.input_str = x['text']
            elif x['type'] == 'image_url':
                image_url = x['image_url']['url']
                if image_url.startswith("data:"):
                    try:
                        base64_data = image_url.split(",")[1]
                        img_bytes = base64.b64decode(base64_data)
                        img = Image.open(BytesIO(img_bytes))
                        ext = img.format.lower() if img.format else "jpg"
                        save_path = os.path.join(tmp_dir, f"{uuid.uuid4()}.{ext}")
                        img.save(save_path, format=img.format or "JPEG")
                        slm.image_str.append(save_path)
                    except Exception as e:
                        print(f"图片解码失败: {e}")
                else:
                    src_ext = os.path.splitext(image_url.split('/')[-1])[1]
                    ext = src_ext if src_ext else ".jpg"
                    save_path = os.path.join(tmp_dir, f"{uuid.uuid4()}{ext}")
                    os.system(f"wget {image_url} -O {save_path}")
                    slm.image_str.append(save_path)
    elif isinstance(request.messages[-1]['content'], str):
        if len(request.messages[-1]['content']) > slm.model.SEQLEN:
            request.messages[-1]['content'] = request.messages[-1]['content'][:slm.model.SEQLEN]
        slm.input_str = request.messages[-1]['content']
    else:
        return JSONResponse({"error": "Invalid content type"}, status_code=400)

    # 编码
    if slm.image_str:
        missing_images = [x for x in slm.image_str if not os.path.exists(x)]
        if missing_images:
            print(f"Missing images: {missing_images}")
            slm.encode()
        else:
            slm.patch_num = len(slm.image_str)
            slm.encode_with_image()
    else:
        slm.encode()

    # 推理
    if request.stream:
        def generate_responses():
            token = slm.model.forward_first(slm.input_ids, slm.pixel_values, slm.image_offsets, slm.patch_num)
            output_tokens = []
            while token not in slm.ID_EOS and slm.model.token_length < slm.model.SEQLEN:
                output_tokens.append(token)
                word = slm.tokenizer.decode(output_tokens, skip_special_tokens=True)
                if "�" not in word:
                    if len(output_tokens) == 1:
                        pre_word = word
                        word = slm.tokenizer.decode([token, token], skip_special_tokens=True)[len(pre_word):]
                    data = {"object":"chat.completion",
                            "created":int(time.time()),
                            "model":"MiniCPM-V2.6",
                            "choices": [{"delta": {"role": "assistant", "content": word}}]}
                    yield f"data:{json.dumps(data)}\n\n"
                    output_tokens = []
                token = slm.model.forward_next()
        return StreamingResponse(generate_responses(), media_type="text/event-stream")
    else:
        token = slm.model.forward_first(slm.input_ids, slm.pixel_values, slm.image_offsets, slm.patch_num)
        output_tokens = [token]
        while token not in slm.ID_EOS and slm.model.token_length < slm.model.SEQLEN:
            token = slm.model.forward_next()
            output_tokens.append(token)
        answer = slm.tokenizer.decode(output_tokens)
        data = {"object":"chat.completion",
                "created":int(time.time()),
                "model":"MiniCPM-V2.6",
                "choices": [{"delta": {"role": "assistant", "content": answer}}]}
        return JSONResponse(data)
```

## Checklist

<table border="1" style="border-collapse: collapse; text-align: center; width: 100%;">
  <thead>
    <tr>
      <th>Cat.</th>
      <th>编号</th>
      <th>Check 项目</th>
      <th>Check 方法与参考值</th>
      <th>Check 结果</th>
      <th>RP & Time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="2">部署bmodel</td>
      <td>1</td>
      <td>安装sail/tpu_perf/untool</td>
      <td>是否成功安装<br>参考值：是</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>2</td>
      <td>运行示例代码</td>
      <td>替换正确文件路径、运行无报错<br>参考值：是</td>
      <td></td>
      <td></td>
    </tr>
        <td rowspan="2">搭建api服务</td>
      <td>3</td>
      <td>环境配置</td>
      <td>安装untool、fastapi<br>参考值：是</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>4</td>
      <td>运行fastapi服务</td>
      <td>运行是否报错<br>参考值：否</td>
      <td></td>
      <td></td>
    </tr>
  </tbody>
</table>
